{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6XbgH5SrNXm",
        "outputId": "a8fad525-cd84-435a-a6e3-b2ce6b545d38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w 0\n",
            "[[0.75 0.75 0.75 0.75]\n",
            " [0.75 0.75 0.75 0.75]]\n",
            "b 0\n",
            "[[0.25 0.25 0.25 0.25]]\n",
            "w 1\n",
            "[[0.75 0.75 0.75 0.75 0.75 0.75 0.75 0.75]\n",
            " [0.75 0.75 0.75 0.75 0.75 0.75 0.75 0.75]\n",
            " [0.75 0.75 0.75 0.75 0.75 0.75 0.75 0.75]\n",
            " [0.75 0.75 0.75 0.75 0.75 0.75 0.75 0.75]]\n",
            "b 1\n",
            "[[0.2466734 0.2466734 0.2466734 0.2466734 0.2466734 0.2466734 0.2466734\n",
            "  0.2466734]]\n",
            "w 2\n",
            "[[0.26303472]\n",
            " [0.26303472]\n",
            " [0.26303472]\n",
            " [0.26303472]\n",
            " [0.26303472]\n",
            " [0.26303472]\n",
            " [0.26303472]\n",
            " [0.26303472]]\n",
            "b 2\n",
            "[[0.24345709]]\n"
          ]
        }
      ],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "sigmoid = (\n",
        "  lambda x:1 / (1 + np.exp(-x)),\n",
        "  lambda x:x * (1 - x)\n",
        "  )\n",
        "\n",
        "def derivada_relu(x):\n",
        "  x[x<=0] = 0\n",
        "  x[x>0] = 1\n",
        "  return x\n",
        "\n",
        "relu = (\n",
        "  lambda x: x * (x > 0),\n",
        "  lambda x:derivada_relu(x)\n",
        "  )\n",
        "\n",
        "def circulo(num_datos = 100,R = 1, minimo = 0,maximo= 1):\n",
        "  pi = math.pi\n",
        "  r = R * np.sqrt(stats.truncnorm.rvs(minimo, maximo, size= num_datos)) * 10\n",
        "  theta = stats.truncnorm.rvs(minimo, maximo, size= num_datos) * 2 * pi *10\n",
        "  #print(len(r))\n",
        "  #print(len(theta))\n",
        "  x = np.cos(theta) * r\n",
        "  y = np.sin(theta) * r\n",
        "\n",
        "  y = y.reshape((num_datos,1))\n",
        "  x = x.reshape((num_datos,1))\n",
        "\n",
        "  #Vamos a reducir el numero de elementos para que no cause un Overflow\n",
        "  x = np.round(x,3)\n",
        "  y = np.round(y,3)\n",
        "\n",
        "  df = np.column_stack([x,y])\n",
        "  #print(df)\n",
        "  return(df)\n",
        "\n",
        "#print(len(X))\n",
        "#print(len(Y))\n",
        "# Numero de neuronas en cada capa.\n",
        "# El primer valor es el numero de columnas de la capa de entrada.\n",
        "#neuronas = [2,4,8,1]\n",
        "\n",
        "# Funciones de activacion usadas en cada capa.\n",
        "#funciones_activacion = [relu,relu, sigmoid]\n",
        "\n",
        "#red_neuronal = []\n",
        "\n",
        "#for paso in range(len(neuronas)-1):\n",
        "#  x = capa(neuronas[paso],neuronas[paso+1],funciones_activacion[paso])\n",
        "#  print(neuronas[paso],neuronas[paso+1],funciones_activacion[paso])\n",
        "\n",
        "#  red_neuronal.append(x)\n",
        "#  print(x)\n",
        "\n",
        "#print(red_neuronal)\n",
        "\n",
        "\n",
        "def mse(Ypredich, Yreal):\n",
        "\n",
        "  # Calculamos el error\n",
        "  #print(Ypredich)\n",
        "  #print(Yreal)\n",
        "  x = (np.array(Ypredich) - np.array(Yreal)) ** 2\n",
        "  #print(x)\n",
        "  x = np.mean(x)\n",
        "  #print(x)\n",
        "  # Calculamos la derivada de la funcion\n",
        "  y = np.array(Ypredich) - np.array(Yreal)\n",
        "  return (x,y)\n",
        "\n",
        "\n",
        "def entrenamiento(epoch, X,Y, red_neuronal, lr = 0.01):\n",
        "\n",
        "  # Output guardara el resultado de cada capa\n",
        "  # En la capa 1, el resultado es el valor de entrada\n",
        "  output = [X]\n",
        "  for num_capa in range(len(red_neuronal)):\n",
        "    z = output[-1] @ red_neuronal[num_capa].W + red_neuronal[num_capa].b\n",
        "    a = red_neuronal[num_capa].funcion_act[0](z)\n",
        "    #print(epoch)\n",
        "    #if (epoch==0) or (epoch==-999):\n",
        "      #print(epoch)\n",
        "      #print(red_neuronal[num_capa].funcion_act[0]( np.array([-1, 1]) ))\n",
        "      #print(X, X.shape)\n",
        "      #print('w', red_neuronal[num_capa].W.shape)\n",
        "      #print(red_neuronal[num_capa].W)\n",
        "      #print('b', red_neuronal[num_capa].b.shape)\n",
        "      #print(red_neuronal[num_capa].b)\n",
        "\n",
        "      #print('z', z.shape)\n",
        "      #print(z)\n",
        "      #print('act ', num_capa)\n",
        "      #print(a)\n",
        "    # Incluimos el resultado de la capa a output\n",
        "    output.append(a)\n",
        "\n",
        "  # Backpropagation\n",
        "\n",
        "  back = list(range(len(output)-1))\n",
        "  back.reverse()\n",
        "\n",
        "  # Guardaremos el error de la capa en delta\n",
        "  delta = []\n",
        "\n",
        "  for capa in back:\n",
        "    # Backprop #delta\n",
        "\n",
        "    a = output[capa+1]\n",
        "    #if (epoch==0) or (epoch==-999):\n",
        "    #  print('activation', a)\n",
        "\n",
        "    if capa == back[0]:\n",
        "      x = mse(a,Y)[1] * red_neuronal[capa].funcion_act[1](a)\n",
        "      delta.append(x)\n",
        "      #if (epoch==0) and (epoch==-999):\n",
        "      #  print('mse 0', x)\n",
        "        #print('mse 01', mse(a,Y)[1] )\n",
        "        #print('mse 02', red_neuronal[capa].funcion_act[1](a) )\n",
        "        #print('mse 02', red_neuronal[capa].funcion_act[1](np.array([-2, -1, 0, 1, 2])) )\n",
        "\n",
        "    else:\n",
        "      #if (epoch==0) and (capa == back[1]):\n",
        "      #  print('function a ', a)\n",
        "      #  print('delta', delta[-1])\n",
        "      #  print('W_temp', W_temp)\n",
        "\n",
        "      x = delta[-1] @ W_temp * red_neuronal[capa].funcion_act[1](a)\n",
        "      #if (epoch==0) and (capa == back[1]):\n",
        "      #  print('x ', x)\n",
        "\n",
        "      delta.append(x)\n",
        "\n",
        " #       print('mse 2', x)\n",
        "\n",
        "    W_temp = red_neuronal[capa].W.transpose()\n",
        "    #if (epoch==0) or (epoch==-999):\n",
        "    #  print('W_temp', capa, red_neuronal[capa].W, W_temp)\n",
        "    # Gradient Descent #\n",
        "    red_neuronal[capa].b = red_neuronal[capa].b - np.mean(delta[-1], axis = 0, keepdims = True) * lr\n",
        "    red_neuronal[capa].W = red_neuronal[capa].W - output[capa].transpose() @ delta[-1] * lr\n",
        "    #if (epoch==0) and (capa==0):\n",
        "    #  print('output 0', output[capa].transpose())\n",
        "    #  print('delta  0', delta[-1])\n",
        "\n",
        "    #print(red_neuronal[capa].W)\n",
        "    #print('b', capa)\n",
        "    #print(red_neuronal[capa].b)\n",
        "\n",
        "  return output[-1]\n",
        "\n",
        "class capa():\n",
        "  def __init__(self, n_neuronas_capa_anterior, n_neuronas, funcion_act):\n",
        "    self.funcion_act = funcion_act\n",
        "    #self.b  = np.round(stats.truncnorm.rvs(-1, 1, loc=0, scale=1, size= n_neuronas).reshape(1,n_neuronas),3)\n",
        "    #self.W  = np.round(stats.truncnorm.rvs(-1, 1, loc=0, scale=1, size= n_neuronas * n_neuronas_capa_anterior).reshape(n_neuronas_capa_anterior,n_neuronas),3)\n",
        "    self.b  = np.full((1,n_neuronas), 0.25).reshape(1,n_neuronas)\n",
        "    self.W  = np.full((1,n_neuronas * n_neuronas_capa_anterior), 0.75).reshape(n_neuronas_capa_anterior,n_neuronas)\n",
        "\n",
        "\n",
        "N=150\n",
        "\n",
        "datos_1 = circulo(num_datos = N, R = 2)\n",
        "datos_2 = circulo(num_datos = N, R = 0.5)\n",
        "#X = np.concatenate([datos_1,datos_2])\n",
        "#X = np.round(X,3)\n",
        "\n",
        "X = np.array([ [19.467222027313564, -99.12219861354929],\n",
        "[19.482265616680536, -99.14190350018883],\n",
        "[19.476990639248996, -99.13936414016929],\n",
        "[19.462926665702092,  -99.14510100533802],\n",
        "[19.465871557932942, -99.1367188182426],\n",
        "[19.47237365730388, -99.14509978671352],\n",
        "[19.464066506396268, -99.13291715146295],\n",
        "[19.462052618719053, -99.15092596673614],\n",
        "[19.474422807391647,  -99.1438514157048],\n",
        "[19.47237241832461,  -99.14570383179539],\n",
        "[19.450085377691707,  -99.12859837428849],\n",
        "[19.479921790581052,  -99.14667476345326],\n",
        "[19.453633512514266, -99.13628285667163],\n",
        "[19.472190658390566, -99.14982956004451],\n",
        "[19.47395263950297,  -99.15101797733233],\n",
        "[19.465252402898983, -99.13898902224867],\n",
        "[19.461874805051114, -99.14515853743147],\n",
        "[19.469209782455476, -99.14038136552176],\n",
        "[19.45741613536277, -99.13686213058105],\n",
        "[19.47421974621611,  -99.14489828681003],\n",
        "[19.4662759012438, -99.14030630674392],\n",
        "[19.448275401978687, -99.14286578386036],\n",
        "[19.469434885003373, -99.12735700514285],\n",
        "[19.4749361786807, -99.12654419193377],\n",
        "[19.47997289288032, -99.1420344373171],\n",
        "[19.46620589351966, -99.1399694994465],\n",
        "[19.476580907985532, -99.11686758601223],\n",
        "[19.461851297630492, -99.11433631660428],\n",
        "[19.47705057940435, -99.13037438935557],\n",
        "[19.470577377784227, -99.14363811186868],\n",
        "[19.485601032367263, -99.13049711986271],\n",
        "[19.48488889230067, -99.14022466504971],\n",
        "[19.485374180852496, -99.14287379001047],\n",
        "[19.456310798085966, -99.13839232623272],\n",
        "[19.48251305339346, -99.15563537453852],\n",
        "[19.46434471028183, -99.15938312521207],\n",
        "[19.483425965849538, -99.14328489662618],\n",
        "[19.486761037171586, -99.12334701629308],\n",
        "[19.46373747465879, -99.14451466137191],\n",
        "[19.492836626097954, -99.14572553222457],\n",
        "[19.480573183372517, -99.13361184936554],\n",
        "[19.470533864108532, -99.13831201601843],\n",
        "[19.45475663149979, -99.1491266977481],\n",
        "[19.476944101951492, -99.14168910044891],\n",
        "[19.477627432637608, -99.12478839083609],\n",
        "[19.482843060414048, -99.12177061779703],\n",
        "[19.45800204716716, -99.1537745992418],\n",
        "[19.477638425080112, -99.12917924626362],\n",
        "[19.44533463361916, -99.13309049585013],\n",
        "[19.478332736736117, -99.14238543333826],\n",
        "[19.480407845952456,  -99.13212449743668],\n",
        "[19.46952412138314, -99.13212341424693],\n",
        "[19.46307806743916, -99.13591274724371],\n",
        "[19.470269228564263, -99.15138468780052],\n",
        "[19.47141503394753, -99.15805636444716],\n",
        "[19.478786673898128, -99.14103062705644],\n",
        "[19.4600628206952, -99.1487776814693],\n",
        "[19.48606379324133, -99.14890239835681],\n",
        "[19.478509323583307, -99.13233925815327],\n",
        "[19.469865121840453, -99.13786958474182],\n",
        "[19.484410537831664, -99.121693317719],\n",
        "[19.463341465865337, -99.14914795126047],\n",
        "[19.482818358842174, -99.14082501341082],\n",
        "[19.46631819489393, -99.12469562596752],\n",
        "[19.475618269619783, -99.14759375080713],\n",
        "[19.4627645651239, -99.14233753011449],\n",
        "[19.479120452840874, -99.12567981733912],\n",
        "[19.46159647701849, -99.14470594514692],\n",
        "[19.460079605655718, -99.14741174919547],\n",
        "[19.476569829600766, -99.1440625767766],\n",
        "[19.46987021440519, -99.14082658791264],\n",
        "[19.48310472340533, -99.14065797396451],\n",
        "[19.491908108199073, -99.1339835131305],\n",
        "[19.485375544210143, -99.1312665165737],\n",
        "[19.454031977970253, -99.15422455705304],\n",
        "[19.452497279136075, -99.1292192022765],\n",
        "[19.458765579991933, -99.14229789199116],\n",
        "[19.476354588347395, -99.13929907806016],\n",
        "[19.453320804423655, -99.14549547326418],\n",
        "[19.48430362904814, -99.14006115004362],\n",
        "[19.47752750177724, -99.13758299444518],\n",
        "[19.47394999145678, -99.15293220683569],\n",
        "[19.473968111959614, -99.12710937533065],\n",
        "[19.47073051061994, -99.13369236987343],\n",
        "[19.46441825443714, -99.12927833217012],\n",
        "[19.483484533257922, -99.1381563214215],\n",
        "[19.478196301055913, -99.14284986255021],\n",
        "[19.446308912122177, -99.15706179393689],\n",
        "[19.469231275602365, -99.14556403154653],\n",
        "[19.483542086031548, -99.14131650827791],\n",
        "[19.452873444162485, -99.1488118060631],\n",
        "[19.479762408497173, -99.16505535513419],\n",
        "[19.471954180014556, -99.12756997624093],\n",
        "[19.48536406020416, -99.16225064849888],\n",
        "[19.482358721278874,-99.16658050278602],\n",
        "[19.472306443209774, -99.14502501477897],\n",
        "[19.4834665609573, -99.14972179568096],\n",
        "[19.464543085286973, -99.13503426531007],\n",
        "[19.46504234959824, -99.13343340206148],\n",
        "[19.4775589850638, -99.11377193934732],\n",
        "[19.495660202980694, -99.13698606395911],\n",
        "[19.46295217503846, -99.14070137784498],\n",
        "[19.469937918466062, -99.14987218558234],\n",
        "[19.46112726603321, -99.14538049221726],\n",
        "[19.461849710301273, -99.11957394173729],\n",
        "[19.457231790616685, -99.13052646998139],\n",
        "[19.47274660283314, -99.13548709955731],\n",
        "[19.46180004259595, -99.11946493999424],\n",
        "[19.480188978035688,  -99.12285249689992],\n",
        "[19.470279254192434, -99.14223773313448],\n",
        "[19.483522121895632, -99.1428242942711],\n",
        "[19.46169098094029, -99.156447602285],\n",
        "[19.47291209620327, -99.12104964668455],\n",
        "[19.467375582337734, -99.14765867916736],\n",
        "[19.462683049121033, -99.12197320532583],\n",
        "[19.452591058010732, -99.15353925580074],\n",
        "[19.466176632311498, -99.13575398512884],\n",
        "[19.465003853375855, -99.13827391845079],\n",
        "[19.467683711489805, -99.15023940496367],\n",
        "[19.46604724124821, -99.15100221793305],\n",
        "[19.469432182844546, -99.13279593990224],\n",
        "[19.460565817854008, -99.13170909287767],\n",
        "[19.473834734898436, -99.1406158331018],\n",
        "[19.482649974744792, -99.13438872188017],\n",
        "[19.475119593373265, -99.14513081463329],\n",
        "[19.469870302031687, -99.14356074939323],\n",
        "[19.474586644292575, -99.13187908048549],\n",
        "[19.469327348222564, -99.13515743581809],\n",
        "[19.47790601014896, -99.13136064597589],\n",
        "[19.47087371489028, -99.14865738633189],\n",
        "[19.47402729671688, -99.11803399866842],\n",
        "[19.471919731477428, -99.13869444396676],\n",
        "[19.500248069980504, -99.13623631436498],\n",
        "[19.49119403508465, -99.13472739147869],\n",
        "[19.474806930618012, -99.14152343688409],\n",
        "[19.46466055346265, -99.14019832335595],\n",
        "[19.472788390864512, -99.13777561964021],\n",
        "[19.487132603734175, -99.14423144873417],\n",
        "[19.480733763811138, -99.13486790976509],\n",
        "[19.451318874994854, -99.13576177078268],\n",
        "[19.46901379636231, -99.13919133175108],\n",
        "[19.475683358385, -99.13815038927078],\n",
        "[19.4694098141617, -99.11967060662623],\n",
        "[19.45878068199157, -99.15470878657582],\n",
        "[19.472758275068117, -99.1134313956857],\n",
        "[19.46116145706365, -99.14744317356524],\n",
        "[19.46360146156197, -99.15796316868236],\n",
        "[19.44585269034688, -99.12596080046318],\n",
        "[19.46572005748166, -99.12553949026378],\n",
        "[19.45951811724063, -99.15072502741356],\n",
        "[\t48.53384067\t,\t2.617842798\t],\n",
        "[\t48.55115296\t,\t2.600669664\t],\n",
        "[\t48.54813041\t,\t2.606115161\t],\n",
        "[\t48.53583182\t,\t2.611454369\t],\n",
        "[\t48.559793\t,\t2.593962223\t],\n",
        "[\t48.52921008\t,\t2.600309074\t],\n",
        "[\t48.5329234\t,\t2.632414196\t],\n",
        "[\t48.5248124\t,\t2.613880403\t],\n",
        "[\t48.52508281\t,\t2.608197471\t],\n",
        "[\t48.54618928\t,\t2.632532141\t],\n",
        "[\t48.55017127\t,\t2.612889924\t],\n",
        "[\t48.55329165\t,\t2.616240916\t],\n",
        "[\t48.53826385\t,\t2.616131205\t],\n",
        "[\t48.53296758\t,\t2.612200908\t],\n",
        "[\t48.55009432\t,\t2.603417464\t],\n",
        "[\t48.53494555\t,\t2.616196208\t],\n",
        "[\t48.54160127\t,\t2.625886265\t],\n",
        "[\t48.52622097\t,\t2.601108655\t],\n",
        "[\t48.53990201\t,\t2.599727329\t],\n",
        "[\t48.54242181\t,\t2.614200374\t],\n",
        "[\t48.552599\t,\t2.618122468\t],\n",
        "[\t48.56590669\t,\t2.607144935\t],\n",
        "[\t48.54134209\t,\t2.612875555\t],\n",
        "[\t48.54503743\t,\t2.624234055\t],\n",
        "[\t48.53197892\t,\t2.62154926\t],\n",
        "[\t48.53550167\t,\t2.611253737\t],\n",
        "[\t48.54020758\t,\t2.621302426\t],\n",
        "[\t48.52552935\t,\t2.618951447\t],\n",
        "[\t48.5292991\t,\t2.61063329\t],\n",
        "[\t48.56056865\t,\t2.614341636\t],\n",
        "[\t48.52918456\t,\t2.605839083\t],\n",
        "[\t48.54651169\t,\t2.621330142\t],\n",
        "[\t48.53842127\t,\t2.618265475\t],\n",
        "[\t48.53432506\t,\t2.605732892\t],\n",
        "[\t48.54955589\t,\t2.586225671\t],\n",
        "[\t48.5243268\t,\t2.608772515\t],\n",
        "[\t48.53956295\t,\t2.599834673\t],\n",
        "[\t48.51105944\t,\t2.598971143\t],\n",
        "[\t48.54395737\t,\t2.607186365\t],\n",
        "[\t48.53031823\t,\t2.61155845\t],\n",
        "[\t48.53647776\t,\t2.61007146\t],\n",
        "[\t48.54258374\t,\t2.593789711\t],\n",
        "[\t48.53431442\t,\t2.593348447\t],\n",
        "[\t48.54748668\t,\t2.606374703\t],\n",
        "[\t48.54229094\t,\t2.630238881\t],\n",
        "[\t48.53477394\t,\t2.629015649\t],\n",
        "[\t48.52939976\t,\t2.601539412\t],\n",
        "[\t48.52004916\t,\t2.610331281\t],\n",
        "[\t48.56286461\t,\t2.606454216\t],\n",
        "[\t48.55159687\t,\t2.616337417\t],\n",
        "[\t48.53984464\t,\t2.624141639\t],\n",
        "[\t48.52604818\t,\t2.624000305\t],\n",
        "[\t48.54340964\t,\t2.638943188\t],\n",
        "[\t48.54123961\t,\t2.607130876\t],\n",
        "[\t48.54104207\t,\t2.611851208\t],\n",
        "[\t48.53325229\t,\t2.596489525\t],\n",
        "[\t48.5433949\t,\t2.59340789\t],\n",
        "[\t48.534982\t,\t2.623385524\t],\n",
        "[\t48.53346767\t,\t2.619490833\t],\n",
        "[\t48.53116627\t,\t2.603992072\t],\n",
        "[\t48.54015414\t,\t2.608244561\t],\n",
        "[\t48.54092784\t,\t2.624908362\t],\n",
        "[\t48.5398258\t,\t2.597574304\t],\n",
        "[\t48.55641658\t,\t2.618147862\t],\n",
        "[\t48.54337243\t,\t2.608763414\t],\n",
        "[\t48.52615408\t,\t2.618201969\t],\n",
        "[\t48.54330798\t,\t2.61260355\t],\n",
        "[\t48.53946122\t,\t2.640925838\t],\n",
        "[\t48.55149401\t,\t2.613307132\t],\n",
        "[\t48.56135061\t,\t2.604478256\t],\n",
        "[\t48.53885218\t,\t2.600737021\t],\n",
        "[\t48.5358474\t,\t2.618817655\t],\n",
        "[\t48.54598594\t,\t2.617689781\t],\n",
        "[\t48.53501891\t,\t2.610599727\t],\n",
        "[\t48.53763575\t,\t2.610894493\t],\n",
        "[\t48.53732356\t,\t2.609436313\t],\n",
        "[\t48.53594999\t,\t2.608506556\t],\n",
        "[\t48.55031254\t,\t2.613848245\t],\n",
        "[\t48.54431132\t,\t2.615636437\t],\n",
        "[\t48.54002501\t,\t2.611736172\t],\n",
        "[\t48.52011725\t,\t2.593790632\t],\n",
        "[\t48.54046159\t,\t2.602653724\t],\n",
        "[\t48.53415504\t,\t2.61265933\t],\n",
        "[\t48.53029397\t,\t2.618245445\t],\n",
        "[\t48.53545931\t,\t2.60312646\t],\n",
        "[\t48.54686619\t,\t2.597532983\t],\n",
        "[\t48.54107798\t,\t2.607280066\t],\n",
        "[\t48.54881477\t,\t2.614390014\t],\n",
        "[\t48.54228478\t,\t2.607324119\t],\n",
        "[\t48.54201961\t,\t2.610985464\t],\n",
        "[\t48.54283699\t,\t2.615038748\t],\n",
        "[\t48.54376187\t,\t2.616549999\t],\n",
        "[\t48.55128933\t,\t2.606038894\t],\n",
        "[\t48.54423832\t,\t2.610428913\t],\n",
        "[\t48.53256951\t,\t2.604212426\t],\n",
        "[\t48.5320656\t,\t2.604583687\t],\n",
        "[\t48.54409272\t,\t2.638649864\t],\n",
        "[\t48.5514997\t,\t2.611232202\t],\n",
        "[\t48.53746568\t,\t2.607886336\t],\n",
        "[\t48.53471379\t,\t2.628613672\t],\n",
        "[\t48.53539181\t,\t2.603766975\t],\n",
        "[\t48.52281858\t,\t2.622096467\t],\n",
        "[\t48.54275383\t,\t2.613571291\t],\n",
        "[\t48.53808075\t,\t2.625956095\t],\n",
        "[\t48.54475149\t,\t2.623037493\t],\n",
        "[\t48.55530759\t,\t2.60753\t],\n",
        "[\t48.5371911\t,\t2.605349154\t],\n",
        "[\t48.54797101\t,\t2.603331719\t],\n",
        "[\t48.53487689\t,\t2.602290789\t],\n",
        "[\t48.53119854\t,\t2.60174092\t],\n",
        "[\t48.5418094\t,\t2.621086763\t],\n",
        "[\t48.53697317\t,\t2.601252107\t],\n",
        "[\t48.54125637\t,\t2.609791619\t],\n",
        "[\t48.53372287\t,\t2.612387855\t],\n",
        "[\t48.55283425\t,\t2.608123374\t],\n",
        "[\t48.53808367\t,\t2.615505203\t],\n",
        "[\t48.53225517\t,\t2.600340025\t],\n",
        "[\t48.54938105\t,\t2.631928007\t],\n",
        "[\t48.54238848\t,\t2.624814728\t],\n",
        "[\t48.5271638\t,\t2.598368582\t],\n",
        "[\t48.54088154\t,\t2.605746112\t],\n",
        "[\t48.54417127\t,\t2.611542869\t],\n",
        "[\t48.54169212\t,\t2.59920863\t],\n",
        "[\t48.54297979\t,\t2.614172614\t],\n",
        "[\t48.55451867\t,\t2.624969916\t],\n",
        "[\t48.54013272\t,\t2.623996907\t],\n",
        "[\t48.53573374\t,\t2.613994653\t],\n",
        "[\t48.5200309\t,\t2.60802105\t],\n",
        "[\t48.54560615\t,\t2.607420477\t],\n",
        "[\t48.54585471\t,\t2.616360689\t],\n",
        "[\t48.52540245\t,\t2.602603841\t],\n",
        "[\t48.53987204\t,\t2.603320273\t],\n",
        "[\t48.5532735\t,\t2.604968681\t],\n",
        "[\t48.51808581\t,\t2.613234121\t],\n",
        "[\t48.56152477\t,\t2.616037593\t],\n",
        "[\t48.55386902\t,\t2.621139161\t],\n",
        "[\t48.53176506\t,\t2.611822468\t],\n",
        "[\t48.55172991\t,\t2.613272924\t],\n",
        "[\t48.55016767\t,\t2.617837218\t],\n",
        "[\t48.53475022\t,\t2.609472833\t],\n",
        "[\t48.54709553\t,\t2.616779019\t],\n",
        "[\t48.54656873\t,\t2.611303596\t],\n",
        "[\t48.52371716\t,\t2.611986677\t],\n",
        "[\t48.54367431\t,\t2.614694333\t],\n",
        "[\t48.53012162\t,\t2.634398603\t],\n",
        "[\t48.53188729\t,\t2.608154232\t],\n",
        "[\t48.54428816\t,\t2.602299928\t],\n",
        "[\t48.52500598\t,\t2.614197207\t],\n",
        "[\t48.5465352\t,\t2.608028039\t],\n",
        "[\t48.55526452\t,\t2.612763409\t]\n",
        "             ])\n",
        "\n",
        "Y = [0] * N + [1] * N\n",
        "Y = np.array(Y).reshape(len(Y),1)\n",
        "\n",
        "#print('labels')\n",
        "#print(Y)\n",
        "\n",
        "neuronas = [2,4,8,1]\n",
        "funciones_activacion = [relu,relu, sigmoid]\n",
        "red_neuronal = []\n",
        "\n",
        "for paso in list(range(len(neuronas)-1)):\n",
        "  x = capa(neuronas[paso],neuronas[paso+1],funciones_activacion[paso])\n",
        "  red_neuronal.append(x)\n",
        "  #print('w')\n",
        "  #print(neuronas[paso])\n",
        "  #print(neuronas[paso+1])\n",
        "\n",
        "  #print(x.W)\n",
        "  #print('b')\n",
        "  #print(x.b)\n",
        "  #print('---')\n",
        "error = []\n",
        "predicciones = []\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "  ronda = entrenamiento(epoch, X = X ,Y = Y ,red_neuronal = red_neuronal, lr = 0.001)\n",
        "  #print('-----')\n",
        "  #print(ronda)\n",
        "  #print(Y)\n",
        "  predicciones.append(ronda)\n",
        "  temp = mse(np.round(predicciones[-1]),Y)[0]\n",
        "  #print(temp)\n",
        "  #print('-----')\n",
        "  error.append(temp)\n",
        "\n",
        "\n",
        "#print('===final Y1 =====')\n",
        "#print(predicciones[-1][0:N])\n",
        "#print('===final Y2 =====')\n",
        "#print(predicciones[-1][N:N*2])\n",
        "\n",
        "#print(Y)\n",
        "\n",
        "print('w 0')\n",
        "print(red_neuronal[-3].W)\n",
        "print('b 0')\n",
        "print(red_neuronal[-3].b)\n",
        "\n",
        "print('w 1')\n",
        "print(red_neuronal[-2].W)\n",
        "print('b 1')\n",
        "print(red_neuronal[-2].b)\n",
        "\n",
        "\n",
        "print('w 2')\n",
        "print(red_neuronal[-1].W)\n",
        "print('b 2')\n",
        "print(red_neuronal[-1].b)\n",
        "\n",
        "\n",
        "\n",
        "#epoch = list(range(0,1))\n",
        "#plt.cla()\n",
        "#plt.plot(epoch, error)],"
      ]
    }
  ]
}